{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac2db87-4f47-4881-9f9e-bbef229d7de4",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = logo-IJCLab-1.png width=250 style=\"padding: 10px\"> \n",
    "\n",
    "**Notebook Author**: Martín Rodríguez Monroy\n",
    "\n",
    "**Date last tested**: 2022-10-13\n",
    "\n",
    "**Ran with RSP image**: Weekly 2022_40\n",
    "\n",
    "**A large container is recommended for this notebook.**\n",
    "\n",
    "This notebook demonstrates how to bin and do Principle Component Analysis (PCA) with survey property maps, and how to create a galaxy number map using the Object catalog.\n",
    "\n",
    "This notebook builds off DP0.2 Tutorial Notebook 03c_Survey_Property_Maps.ipynb, available in the <a href=\"https://github.com/rubin-dp0/tutorial-notebooks\">tutorial-notebooks repository</a>.\n",
    "Data products are accessed through the Butler, and the user is expected to be familiar with the content of the introductory Butler tutorial in that repo (04a_Introduction_to_the_Butler.ipynb).\n",
    "\n",
    "**Useful documentation**\n",
    "\n",
    "<a href=\"https://healsparse.readthedocs.io/en/latest/index.html\">HealSparse documentation</a>\n",
    "\n",
    "<a href=\"https://buildmedia.readthedocs.org/media/pdf/python-for-multivariate-analysis/latest/python-for-multivariate-analysis.pdf\">Principal Component Analysis notes</a>\n",
    "\n",
    "**Credit:** Developed by Martín Rodríguez Monroy with the help and editing of Sylvie Dagoret-Campagne and Melissa Graham. This notebook is based in part on material originally developed by Eli Rykoff for the DP0.2 Tutorial Notebook 03c_Survey_Property_Maps.ipynb\n",
    "\n",
    "\n",
    "# 1.0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9a67f-57c6-4295-8192-19ce359fa954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general python packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import ZScaleInterval, LinearStretch, ImageNormalize\n",
    "from astropy.wcs import WCS\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# specific packages for statistics and principal component analysis\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# packages for working with sparse healpix maps\n",
    "import healsparse as hsp\n",
    "import skyproj\n",
    "\n",
    "#packages for working with healpy healpix maps\n",
    "import healpy as hp\n",
    "\n",
    "# LSST packages\n",
    "from lsst.daf.butler import Butler\n",
    "import lsst.geom as geom\n",
    "\n",
    "# allow interactive plots\n",
    "%matplotlib widget\n",
    "\n",
    "# default plot style is accessible\n",
    "plt.style.use('tableau-colorblind10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e50be-f338-4f97-afe0-23f4f7bf85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'dp02'\n",
    "collections = '2.2i/runs/DP0.2'\n",
    "butler = Butler(config, collections=collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db720151-c686-445d-a793-a484f3b45deb",
   "metadata": {},
   "source": [
    "# 2.0. Display the map of magnitude limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e857a-db19-4807-b888-a06d7df449eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hspmap = butler.get('deepCoadd_psf_maglim_consolidated_map_weighted_mean', band='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62a778-01ff-449f-950f-43cb93231073",
   "metadata": {},
   "outputs": [],
   "source": [
    "nside_coverage = hspmap.nside_coverage\n",
    "nside_sparse = hspmap.nside_sparse\n",
    "print('nside_coverage = ', hspmap.nside_coverage)\n",
    "print('nside_sparse = ', hspmap.nside_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e798eb6-f8b1-4bf6-8a63-112da51a2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sp = skyproj.McBrydeSkyproj(ax=ax, lon_0=65.0)\n",
    "sp.draw_hspmap(hspmap, vmin=26.0, vmax=26.3)\n",
    "sp.draw_colorbar(label='PSF Maglim (i-band)')\n",
    "plt.show()\n",
    "\n",
    "del fig, ax, sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a0b0b-2406-49f9-829e-dfc52a9ed3be",
   "metadata": {},
   "source": [
    "# 3.0. Degrade the SP map to lower nside resolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7037b-2782-4b5a-9084-f66bd4320025",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_nside = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2280f5-d849-408b-bafc-b4a7653ae92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hspmap = hspmap.degrade(deg_nside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab808964-9ed5-4b94-ade6-aec83fc21f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nside_coverage = ', hspmap.nside_coverage)\n",
    "print('nside_sparse = ', hspmap.nside_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0880cae-462a-4e12-bec2-b9af037c7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sp = skyproj.McBrydeSkyproj(ax=ax, lon_0=65.0)\n",
    "sp.draw_hspmap(hspmap, vmin=26.0, vmax=26.3)\n",
    "sp.draw_colorbar(label='PSF Maglim (i-band)')\n",
    "plt.show()\n",
    "\n",
    "del fig, ax, sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9d804-0a75-4ffb-9396-a7ef8ed11f6c",
   "metadata": {},
   "source": [
    "# 4.0. Binning the SP maps on sky \n",
    "The idea is to bin the value distribution of the SP map to later evaluate $n_{gal}$ on those bins \n",
    "projected on the sky. SP maps with very skewed distributions (with long tails) can result in bins \n",
    "with very low statistics (few pixels lying on them) or even empty bins if using an equal width binning. \n",
    "Then, it is useful to use an equal area binning, which ensures that each SP bin covers similar areas \n",
    "on the sky and therefore contain similar statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2c663-4e06-4991-b8f7-29b8cfa6aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_area_bin_edges(map_data,nbins):\n",
    "        data_min = map_data.min()\n",
    "        data_max = map_data.max()\n",
    "        pix_per_bin = int(len(map_data)/nbins)\n",
    "\n",
    "        data_sort = np.sort(map_data)\n",
    "        if nbins*pix_per_bin==len(map_data):\n",
    "                data_sort = np.append(data_sort,data_max)\n",
    "        binedges = [data_sort[i*pix_per_bin] for i in range(nbins+1)]\n",
    "        binedges[-1] = data_max\n",
    "\n",
    "        if len(np.unique(binedges)) != len(binedges):\n",
    "                raise RuntimeError('Your bin edges are not unique please set them manually')\n",
    "\n",
    "        return binedges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ca7f3-3a02-4eae-a2cc-147f71a6b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pixels where there are actual SP measurements \n",
    "valid_pix = hspmap.valid_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260f2b0-0858-4982-95a7-500ffeb0687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also possible to obtain the RA,DEC coordinates of the center of the valid pixels. This is useful \n",
    "# for generating additional masks \n",
    "sp_ra, sp_dec = hspmap.valid_pixels_pos(lonlat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7017e-8cc0-4e6a-9a3d-3c064fe8dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values of the SP map on the valid pixels (otherwise, the pixel contains the sentinel value hp.UNSEEN\n",
    "vals = hspmap.get_values_pix(valid_pix, nest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16de4d6-aa2a-44fb-a765-4a47384a0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks to ensure the outputs make sense \n",
    "print(len(valid_pix),len(vals),len(sp_ra))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e305d7c-3927-47ee-abb9-48191ae7ccc8",
   "metadata": {},
   "source": [
    "Let's compute the equal area bins for our reference SP map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a498d0b-0a3a-44a9-b3d1-e36b76dbda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges1d = equal_area_bin_edges(vals,nbins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f33ef-26fc-4071-a603-f69bf15046c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "_ = plt.hist(vals,bins=100)\n",
    "for spbin in binedges1d:\n",
    "    plt.axvline(x=spbin,ls='--',color='orange')\n",
    "plt.plot([],[],ls='--',color='orange',label='Bin edges')\n",
    "plt.grid()\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('SP values')\n",
    "plt.ylabel('Number of pixels')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407cd630-8283-4a0e-aea1-ba603651b09a",
   "metadata": {},
   "source": [
    "To better visualize this process, we can project these bins on the sky "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21656341-34e5-4259-9be2-1a5c6e861e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_bins = np.ones(len(valid_pix))\n",
    "for ibin in range(len(binedges1d)-1):\n",
    "    ibin_mask = (vals>binedges1d[ibin])*(vals<binedges1d[ibin+1])\n",
    "    sky_bins[ibin_mask] = ibin+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf553d-8558-4564-9462-7709c8a50504",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsp_bins = hsp.HealSparseMap.make_empty(hspmap.nside_coverage, hspmap.nside_sparse, dtype=np.float64)\n",
    "hsp_bins.update_values_pix(valid_pix, sky_bins,operation='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412210c-b359-4554-9622-d259c2a41f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sp = skyproj.McBrydeSkyproj(ax=ax, lon_0=65.0)\n",
    "sp.draw_hspmap(hsp_bins)\n",
    "sp.draw_colorbar(label='PSF Maglim (i-band)')\n",
    "plt.show()\n",
    "\n",
    "del fig, ax, sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a0622-40fb-4192-a90c-283d2d03ad32",
   "metadata": {},
   "source": [
    "Let's load additional SP maps at nside = 512. In the tutorial 03c_Survey_Property_Maps we can learn how \n",
    "to check the available SP maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fff23d-4e8b-404d-9d06-4683452bce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_names = []\n",
    "for dtype in sorted(butler.registry.queryDatasetTypes(expression=\"*consolidated_map*\")):\n",
    "    print(dtype.name)\n",
    "    sp_names.append(dtype.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915bd81-4c3d-4f66-ac75-8b3fe0d57912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell to run with a smaller set of maps \n",
    "'''\n",
    "sp_names = ['deepCoadd_exposure_time_consolidated_map_sum',\n",
    "            'deepCoadd_psf_maglim_consolidated_map_weighted_mean',\n",
    "            'deepCoadd_psf_size_consolidated_map_weighted_mean',\n",
    "            'deepCoadd_sky_background_consolidated_map_weighted_mean']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce7add-bba9-4b90-a8a3-f8f7221329d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create some short names for each SP map \n",
    "short_names = {}\n",
    "for sp_name in sp_names:\n",
    "    name_ = sp_name.replace('deepCoadd_','')\n",
    "    if 'weighted_mean' in name_:\n",
    "        name_ = name_.replace('_consolidated_map_weighted_mean','')\n",
    "    elif 'sum' in name_:\n",
    "        name_ = name_.replace('_consolidated_map_sum','')\n",
    "    short_names[sp_name] = name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34085080-d1d7-4518-b720-99ace7ab78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a photometric band \n",
    "band = 'i'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099bb06-4e6e-4609-8a68-f39c32d11841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we generate a dictionary containing necessary SP map information for our next steps. Since this takes a while \n",
    "# (a few minutes), you can find the dictionary already saved in npy format in this repository. In case the dictionary \n",
    "# is not available, you just need to uncomment this cell and generate it\n",
    "'''\n",
    "map_dict = {}\n",
    "for name in sp_names:\n",
    "    hspmap_ = butler.get(name, band='i')\n",
    "    hspmap_ = hspmap_.degrade(deg_nside)\n",
    "    dict_ = {}\n",
    "    dict_['nside_coverage'] = hspmap_.nside_coverage\n",
    "    dict_['nside_sparse'] = hspmap_.nside_sparse\n",
    "    valid_pixels_ = hspmap_.valid_pixels\n",
    "    dict_['valid_pixels'] = valid_pixels_\n",
    "    dict_['map_values'] = np.array(hspmap_.get_values_pix(valid_pixels_, nest=True))\n",
    "    \n",
    "    map_dict[name] = dict_\n",
    "    del hspmap_\n",
    "    \n",
    "np.save('data_dict_sp_maps_nside{0}.npy'.format(deg_nside),map_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154799cc-5b8c-4a89-82d0-54a2aff0a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = np.load('data_dict_sp_maps_nside{0}.npy'.format(deg_nside),allow_pickle=True).ravel()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e05dd-395f-4d4b-ba59-54278e9967d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We verify that all selected SP maps are defined on the same region of the sky \n",
    "for sp in sp_names:\n",
    "    print((map_dict[sp]['valid_pixels']==valid_pix).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb79d4f-e29a-40fa-8ab7-320c4fd874a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict[sp_names[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c345bf7-98d7-4fa9-b7f8-0bbd69e97ed0",
   "metadata": {},
   "source": [
    "# 5.0 Compute the correlation matrix of the SP maps \n",
    "It is interesting to evaluate the correlation between SP to better understand them. Moreover, one of the \n",
    "main purposes of these maps is to correct for their impact on the data, so it is useful to look at the \n",
    "correlations in order to have an idea of whether there are maps than can be excluded "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930976d-fbcb-404c-b3bf-d03207219a3b",
   "metadata": {},
   "source": [
    "# 5.1 Compute correlation matrix based on Pearson's correlation coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96538f8b-6567-41a0-a8d6-bd0203c8c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_p = np.zeros((len(sp_names),len(sp_names)))\n",
    "for i,map_i in enumerate(sp_names):\n",
    "    vals_i = np.array(map_dict[map_i]['map_values'])\n",
    "    for j,map_j in enumerate(sp_names):\n",
    "        vals_j = map_dict[map_j]['map_values']\n",
    "        corr_matrix_p[i,j] = stats.pearsonr(vals_i,vals_j)[0]\n",
    "print(corr_matrix_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38145da-111e-4ceb-be94-890836f83b2b",
   "metadata": {},
   "source": [
    "# 5.2 Compute correlation matrix based on Spearman's correlation coefficient \n",
    "Pearson's correlation coefficient assumes that the random variables have a linear relation between them. \n",
    "We can check this assumption computing the Spearman's correlation coefficient, which only assumes a \n",
    "correlation given by a monotonic function (not necessarily a linear one). If Pearson's and Spearman's \n",
    "coefficients are close, that is a sign of linear correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaea6f9-3bf4-48ac-b74a-c612b42b5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_s = np.zeros((len(sp_names),len(sp_names)))\n",
    "for i,map_i in enumerate(sp_names):\n",
    "    vals_i = np.array(map_dict[map_i]['map_values'])\n",
    "    for j,map_j in enumerate(sp_names):\n",
    "        vals_j = map_dict[map_j]['map_values']\n",
    "        corr_matrix_s[i,j] = stats.spearmanr(vals_i,vals_j)[0]\n",
    "print(corr_matrix_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef0fd2-41c8-4acf-8ff8-7175c2221169",
   "metadata": {},
   "source": [
    "We can plot now the correlation matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33f40f-9f12-4749-a56e-2d606210dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel_matrix = []\n",
    "ylabel_matrix = []\n",
    "for i,sp_name in enumerate(sp_names):\n",
    "    xlabel_matrix.append(short_names[sp_name]+' ({0})'.format(i))\n",
    "    ylabel_matrix.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e1bd9-294f-460a-8260-830ed01d8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8.5,8.5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "sns.heatmap(corr_matrix_p,vmin=-1.0,vmax=1.0,annot=True,cmap='jet',linewidth=0.5,square=True,cbar=True,xticklabels=xlabel_matrix,yticklabels=ylabel_matrix)\n",
    "plt.title(r'$r_P$ coeff, band-{0}'.format(band))\n",
    "\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ee997-ab50-465a-bbc0-a379cd2f3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8.5,8.5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "sns.heatmap(corr_matrix_s,vmin=-1.0,vmax=1.0,annot=True,cmap='jet',linewidth=0.5,square=True,cbar=True,xticklabels=xlabel_matrix,yticklabels=ylabel_matrix)\n",
    "plt.title(r'$r_S$ coeff, band-{0}'.format(band))\n",
    "\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc279716-dfff-4bd4-b17a-0c0ae88199da",
   "metadata": {},
   "source": [
    "# 6.0. Do principal component analysis (PCA) of the SP maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbeaf9-55e7-4a91-9111-73a574b69fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_summary(pca, standardised_data, out=True):\n",
    "        names = [\"PC\"+str(i) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n",
    "        a = list(np.std(pca.transform(standardised_data), axis=0))\n",
    "        b = list(pca.explained_variance_ratio_)\n",
    "        c = [np.sum(pca.explained_variance_ratio_[:i]) for i in range(1,len(pca.explained_variance_ratio_)+1)]\n",
    "        columns = pd.MultiIndex.from_tuples([(\"sdev\", \"Standard deviation\"), (\"varprop\", \"Proportion of Variance\"), (\"cumprop\", \"Cumulative Proportion\")])\n",
    "        summary = pd.DataFrame(zip(a, b, c), index=names, columns=columns)\n",
    "        if out:\n",
    "                print(\"Importance of components:\")\n",
    "                display(summary)\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48caccba-b289-4662-ac08-013f9368033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def screeplot(pca, standardised_values, figsize):\n",
    "        y = np.std(pca.transform(standardised_values), axis=0)**2\n",
    "        x = np.arange(len(y)) + 1\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        fig.set_tight_layout(True)\n",
    "        plt.plot(x, y, \"o-\")\n",
    "        plt.xticks(x, [\"Comp.\"+str(i) for i in x], ha='right', rotation=50, fontsize=12)\n",
    "        plt.ylabel(\"Variance\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d94f2-0d1c-48d0-8442-2b2104d0469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for key in map_dict:\n",
    "    data_dict[key] = map_dict[key]['map_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa4e300-8bc2-4fb8-a555-429984bbe821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8368b23-349a-4a89-ba68-1e4b3b273af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = scale(dataframe)\n",
    "standard_df = pd.DataFrame(standard,columns=dataframe.columns)\n",
    "pca = PCA().fit(standard_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c9aff-a012-4eaf-a106-22b3e50ad498",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pca_summary(pca, standard_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643684fa-7b6b-4a67-89a8-8c6915f445dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv('data_frame.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96249f71-6a2c-486b-95d5-f8688e2bb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot(pca, standard_df, figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80fddf-a0fd-43a3-bc26-83f16de549cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this matrix, each row corresponds to each PC and the values in that row correspond to the coefficients \n",
    "# each original SP map should be multiplied by in order to get that PC map \n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da62b56-0029-42c8-84a6-950998dab6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a8843-e6fc-46cc-ad4e-0ae59216c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the sum of the squared coefficients for each PC map is 1 \n",
    "print(np.sum(pca.components_[0]**2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88efb96-a039-4d67-b862-194a36de776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sp_names)):\n",
    "        pcavalues = pca.transform(standard_df)[:,i]\n",
    "        \n",
    "        hsp_pca_ = hsp.HealSparseMap.make_empty(hspmap.nside_coverage, deg_nside, dtype=np.float64)\n",
    "        hsp_pca_.update_values_pix(valid_pix, pcavalues,operation='replace')\n",
    "        \n",
    "        assert (hsp_pca_.valid_pixels==valid_pix).all()\n",
    "        \n",
    "        # Let's plot some PC maps on the sky \n",
    "        if i<2 or i>len(sp_names)-3:\n",
    "            print('PC {0}'.format(i+1))\n",
    "            fig, ax = plt.subplots(figsize=(8, 5))\n",
    "            sp = skyproj.McBrydeSkyproj(ax=ax, lon_0=65.0)\n",
    "            sp.draw_hspmap(hsp_pca_)\n",
    "            sp.draw_colorbar(label='PSF Maglim (i-band)')\n",
    "            plt.show()\n",
    "            \n",
    "            del fig, ax, sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb438f5-6c31-4889-b7a5-042c47739301",
   "metadata": {},
   "source": [
    "# 7.0. Load galaxies from dp02_dc2_catalogs.Object and create number galaxy map \n",
    "We previously saved the colums that we are interested in from dp02_dc2_catalogs.Object in a pickle file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b195e9d-71e1-4bfd-b3a8-762d0405eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat2map(ra,dec,nside,weight=None):\n",
    "        theta = np.radians(90.-dec)\n",
    "        phi = np.radians(ra)\n",
    "        \n",
    "        map1 = np.zeros(hp.nside2npix(nside))\n",
    "        p = hp.ang2pix(nside,theta,phi,nest=True)\n",
    "        #print(p)\n",
    "        mask = np.zeros(hp.nside2npix(nside)).astype('bool')\n",
    "        mask[p] = True\n",
    "        if weight is None:\n",
    "                for i in p:\n",
    "                        map1[i] += 1\n",
    "        else:\n",
    "                for index,i in enumerate(p):\n",
    "                        map1[i] += weight[index]\n",
    "        map1[~mask] = hp.UNSEEN\n",
    "        pix_ra, pix_dec = hp.pix2ang(nside,np.unique(p),nest=True,lonlat=True)\n",
    "        \n",
    "        return map1, mask, np.unique(p), pix_ra, pix_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43da754-0139-4551-8d4b-2b507a2c89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/mrmonroy/survpropmap/sources_result.pkl','rb') as f:\n",
    "    cat_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4499f4-465e-4b9d-8aab-1a14002c5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c82b0f-d6e1-4322-8ae1-bde8a2fb5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = cat_table['coord_ra']\n",
    "dec = cat_table['coord_dec']\n",
    "refext = cat_table['refExtendedness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851a293-8cea-4512-addc-2950693bc935",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_mask = (refext==1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8879d-10da-45b8-ab9a-7997c68dcfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_ra = np.array(ra[sel_mask])\n",
    "sel_dec = np.array(dec[sel_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e156b7-2b80-4b67-b1d5-cdd1d8b632ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sel_ra))\n",
    "print(sel_ra.min(),sel_ra.max())\n",
    "print(sel_dec.min(),sel_dec.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c24514-de25-4db5-94ae-83e6e4485994",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngal_vals, mask, gal_pixels, pix_ra, pix_dec = cat2map(sel_ra,sel_dec,deg_nside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fae29-1bf6-4c76-877a-467354b5665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.sum(ngal_vals[mask])==len(sel_ra))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7eb8fa-46f5-4927-aaf7-0b5d8babcb63",
   "metadata": {},
   "source": [
    "We can have a look at the galaxy number density distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65afcec3-eec3-4fec-945a-4b175db78e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "n,bins,_ = ax.hist(ngal_vals[mask],bins=100)\n",
    "ax.grid()\n",
    "ax.set_xlabel(r'$n_{gal}$')\n",
    "ax.set_ylabel('Number if pixels')\n",
    "\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f958785-5877-4407-8da3-51ec2e67ee28",
   "metadata": {},
   "source": [
    "Using healSparse we can create a heakpix format map which contains the number of galaxies per pixel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1f57a-4d5b-467a-8a1c-1a419e7e4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsp_map_ngal = hsp.HealSparseMap.make_empty(nside_coverage, deg_nside, dtype=np.float64)\n",
    "pixels_ngal = hp.ang2pix(deg_nside, np.radians(90. - pix_dec), np.radians(pix_ra), nest=True)\n",
    "assert (len(np.unique(pixels_ngal))==len(pixels_ngal))\n",
    "hsp_map_ngal.update_values_pix(pixels_ngal, ngal_vals[mask],operation='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a9d08-4967-4093-80e3-2017582ba5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sp = skyproj.McBrydeSkyproj(ax=ax, lon_0=65.0)\n",
    "sp.draw_hspmap(hsp_map_ngal)\n",
    "sp.draw_colorbar(label='ngal')\n",
    "plt.show()\n",
    "\n",
    "del fig, ax, sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70514537-6cb0-470e-be44-8e1e7ba90001",
   "metadata": {},
   "outputs": [],
   "source": [
    "(hsp_map_ngal.get_values_pix(gal_pixels)==ngal_vals[mask]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9303e36-61e9-407e-980b-1f73a9bed062",
   "metadata": {},
   "source": [
    "Now we evaluate the SP maps in the same regions where the $n_{gal}$ map is defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac1d15-e281-4042-949f-4e9258696454",
   "metadata": {},
   "outputs": [],
   "source": [
    "hspmap_masked = hsp.HealSparseMap.make_empty(nside_coverage, deg_nside, dtype=np.float64)\n",
    "hspmap_masked.update_values_pix(gal_pixels, hspmap.get_values_pix(gal_pixels, nest=True),operation='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488f5f5-4096-4a4c-8884-61a698037518",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sp = skyproj.McBrydeSkyproj(ax=ax, lon_0=65.0)\n",
    "sp.draw_hspmap(hspmap_masked, vmin=26.0, vmax=26.3)\n",
    "sp.draw_colorbar(label='PSF Maglim (i-band)')\n",
    "plt.show()\n",
    "\n",
    "del fig, ax, sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71c8dd-7ec7-4e99-8cfd-ed17b1d68227",
   "metadata": {},
   "source": [
    "Let's mask all the SP maps in preparation for our next step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a402952-4b72-4588-9ac7-fcb21fd55ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this can take a few minutes, so you can find the dictionary with the necessary information in the repository. \n",
    "# If not, you just need to uncomment this cell and run\n",
    "'''\n",
    "masked_map_dict = {}\n",
    "for name in sp_names:\n",
    "    hspmap_ = butler.get(name, band='i')\n",
    "    hspmap_ = hspmap_.degrade(deg_nside)\n",
    "    assert (hspmap_.nside_sparse==hsp_map_ngal.nside_sparse)\n",
    "    \n",
    "    dict_ = {}\n",
    "    dict_['nside_coverage'] = hspmap_.nside_coverage\n",
    "    dict_['nside_sparse'] = hspmap_.nside_sparse\n",
    "    dict_['map_values'] = np.array(hspmap_.get_values_pix(gal_pixels, nest=True))\n",
    "    \n",
    "    masked_map_dict[name] = dict_\n",
    "    del hspmap_\n",
    "    \n",
    "np.save('data_dict_masked_sp_maps_nside{0}.npy'.format(deg_nside),masked_map_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce78f4-a96e-4921-a82f-d7f57d60c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_map_dict = np.load('data_dict_masked_sp_maps_nside{0}.npy'.format(deg_nside),allow_pickle=True).ravel()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579de60-6b69-4fc8-90f2-d156ae513333",
   "metadata": {},
   "source": [
    "# 8.0. Compute 1D relations \n",
    "We define the function to compute them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c920e-219b-4c27-bbf5-ca1fb01fe3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin1d_coords(ngal_map,mask,sp_map_vals,nbins1d):\n",
    "    ngal_vals = ngal_map[mask]\n",
    "    ngal_footprint = np.average(ngal_vals)\n",
    "    \n",
    "    binedges1d = equal_area_bin_edges(sp_map_vals,nbins=nbins1d)\n",
    "    \n",
    "    sp_in_bin1d = []\n",
    "    ngal_in_bin1d = []\n",
    "    err_in_bin1d = []\n",
    "    for ibin in range(len(binedges1d)-1):\n",
    "        ibin_mask = (sp_map_vals>binedges1d[ibin])*(sp_map_vals<binedges1d[ibin+1])\n",
    "        \n",
    "        sp_in_bin1d_ = np.average(sp_map_vals[ibin_mask])\n",
    "        ngal_in_bin1d_ = np.average(ngal_vals[ibin_mask])\n",
    "        err_in_bin1d_ = np.std(ngal_vals[ibin_mask])/np.sqrt(len(ngal_vals[ibin_mask]))\n",
    "        \n",
    "        sp_in_bin1d.append(sp_in_bin1d_)\n",
    "        ngal_in_bin1d.append(ngal_in_bin1d_)\n",
    "        err_in_bin1d.append(err_in_bin1d_)\n",
    "    \n",
    "    sp_in_bin1d = np.array(sp_in_bin1d)\n",
    "    ngal_in_bin1d = np.array(ngal_in_bin1d)\n",
    "    err_in_bin1d = np.array(err_in_bin1d)\n",
    "    \n",
    "    return sp_in_bin1d, ngal_in_bin1d, err_in_bin1d, ngal_footprint\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885e431-57c5-463f-a73a-1e04df7ffd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_fit(x,a,b,c):\n",
    "    return a*x**2.+b*x+c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065359a4-a221-4c23-ae37-2b30d715a9df",
   "metadata": {},
   "source": [
    "Let's plot an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f198f1-0691-43eb-88aa-16657fd0e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins1d = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf1639-4046-450e-b710-34b7051c07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_x,ngal_y,ngal_err,ngal_mean = bin1d_coords(ngal_vals,mask,masked_map_dict[sp_names[0]]['map_values'],nbins1d=nbins1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb000f-4c85-40f1-854d-ccc1c510a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.axhline(y=1.0,ls='--',color='b')\n",
    "ax.plot(sp_x,ngal_y/ngal_mean,color='r')\n",
    "plt.errorbar(sp_x,ngal_y/ngal_mean,yerr=ngal_err/ngal_mean,fmt='.',color='r')\n",
    "ax.grid()\n",
    "ax.set_xlabel(sp_names[0])\n",
    "plt.ylabel(r'$n_{gal}/\\langle n_{gal} \\rangle$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cb7bf-e301-4c68-bf84-2f6d73833d39",
   "metadata": {},
   "source": [
    "Now let's do this for all SP maps and fit each 1D relation with a quadratic function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a992f3-2894-4d33-85e4-2d35bf26779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1d = {}\n",
    "for name in sp_names:\n",
    "    dict_1d_ = {}\n",
    "    \n",
    "    sp_x_,ngal_y_,ngal_err_,ngal_mean_ = bin1d_coords(ngal_vals,mask,masked_map_dict[name]['map_values'],nbins1d=nbins1d)\n",
    "    \n",
    "    p0,cov0 = curve_fit(fun_fit,sp_x_,ngal_y_/ngal_mean_,sigma=ngal_err_/ngal_mean_)\n",
    "    ngal_fit_ = fun_fit(sp_x_,p0[0],p0[1],p0[2])\n",
    "    \n",
    "    dict_1d_['x'] = sp_x_\n",
    "    dict_1d_['y'] = ngal_y_\n",
    "    dict_1d_['err'] = ngal_err_\n",
    "    dict_1d_['ngal_mean'] = ngal_mean_\n",
    "    dict_1d_['yfit'] = ngal_fit_\n",
    "    dict_1d_['fit_coeffs'] = p0\n",
    "    \n",
    "    dict_1d[name] = dict_1d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b732064-8dab-4eb9-a15c-3ca3bb9eb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7b74e-0d2c-4cfa-a809-8e81df489f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16599c7-6e22-445f-8edf-27e489f20b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = 2\n",
    "num_rows = int(len(sp_names)/num_columns)+int(len(sp_names)%num_columns)\n",
    "\n",
    "fig, axs = plt.subplots(num_rows, num_columns, figsize=(8,14))\n",
    "\n",
    "i = 0\n",
    "for row in range(num_rows):\n",
    "    for col in range(num_columns):\n",
    "        if i<len(sp_names):\n",
    "            sp_name = sp_names[i]\n",
    "            i += 1\n",
    "                \n",
    "            ngal_mean_ = dict_1d[sp_name]['ngal_mean']\n",
    "            x_ = dict_1d[sp_name]['x']\n",
    "            y_ = dict_1d[sp_name]['y']/ngal_mean_\n",
    "            err_ = dict_1d[sp_name]['err']/ngal_mean_\n",
    "            yfit_ = dict_1d[sp_name]['yfit']\n",
    "                \n",
    "            axs[row,col].axhline(y=1.0,ls='--',color='b')\n",
    "            axs[row,col].plot(x_,y_,color='r')\n",
    "            axs[row,col].errorbar(x_,y_,yerr=err_,fmt='.',color='r')\n",
    "            axs[row,col].plot(x_,yfit_,ls='--',color='purple')\n",
    "            axs[row,col].grid()\n",
    "            axs[row,col].set_xlabel('SP value')\n",
    "            axs[row,col].set_ylabel(r'$n_{gal}/\\langle n_{gal} \\rangle$')\n",
    "            axs[row,col].set_title(sp_name, fontsize='8')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b86d61-82e4-44e1-98f3-6333bda4c6e9",
   "metadata": {},
   "source": [
    "# 9.0 Derive a weight map from the 1D fits \n",
    "Ideally, we should compute the error bars and the covariance between 1D bins using simulations (computing the \n",
    "1D relation of each SP map with each simulation) or with other techniques, such as Jackknife or Bootstrap. Here, \n",
    "for simplicity, the error bar of each 1D bin is obtained as $\\sigma_{n_{gal}}(bin)/\\sqrt N$, with $N$ the number of \n",
    "pixels in each 1D bin (given the equal area binning, this number is approximately the same for all bins) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f616cf-fa0f-45d0-aa1a-fc4d49b1388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_sp_name = 'deepCoadd_exposure_time_consolidated_map_sum'\n",
    "weight_sp = masked_map_dict[weight_sp_name]['map_values']\n",
    "a_sp = dict_1d[weight_sp_name]['fit_coeffs'][0]\n",
    "b_sp = dict_1d[weight_sp_name]['fit_coeffs'][1]\n",
    "c_sp = dict_1d[weight_sp_name]['fit_coeffs'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90707800-8763-49fe-99c4-a7ea2145f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map = 1./fun_fit(weight_sp,a_sp,b_sp,c_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90203d7-18d7-4ffe-84bd-96e6df643aee",
   "metadata": {},
   "source": [
    "We can plot the value distribution of the weight map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ff0f5-03b1-4566-8c6a-a7ddbc47bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "n,bins,_ = ax.hist(w_map,bins=100)\n",
    "ax.grid()\n",
    "ax.set_xlabel(r'$w$')\n",
    "ax.set_ylabel('Number if pixels')\n",
    "\n",
    "del fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67fb16-1d04-47f6-8dfc-742a122bd4df",
   "metadata": {},
   "source": [
    "Now, we apply the weight map to the $n_{gal}$ map and recompute the 1D relation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b41a42-b25c-4429-b8ef-0cdd5bbbe515",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngal_w = np.copy(ngal_vals)\n",
    "ngal_w[mask] = ngal_vals[mask]*w_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5cfbf3-ee89-4e90-8a6d-dd113b9ac5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_w_x,ngal_w_y,ngal_w_err,ngal_mean_w = bin1d_coords(ngal_w,mask,weight_sp,nbins1d=nbins1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55257e4c-b8ef-430a-9eb8-8bc9d931de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 1D coordinates from before applying weights \n",
    "ngal_mean = dict_1d[weight_sp_name]['ngal_mean']\n",
    "sp_x = dict_1d[weight_sp_name]['x']\n",
    "ngal_y = dict_1d[weight_sp_name]['y']\n",
    "ngal_err = dict_1d[weight_sp_name]['err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd96e54-6df7-4b4d-87dd-d4210b05f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.axhline(y=1.0,ls='--',color='b')\n",
    "ax.plot(sp_x,ngal_y/ngal_mean,color='r',label='Unweighted data')\n",
    "plt.errorbar(sp_x,ngal_y/ngal_mean,yerr=ngal_err/ngal_mean,fmt='.',color='r')\n",
    "ax.plot(sp_w_x,ngal_w_y/ngal_mean_w,color='b',label='Weighted data')\n",
    "plt.errorbar(sp_w_x,ngal_w_y/ngal_mean_w,yerr=ngal_w_err/ngal_mean_w,fmt='.',color='b')\n",
    "ax.grid()\n",
    "ax.set_xlabel(weight_sp_name)\n",
    "ax.set_ylabel(r'$n_{gal}/\\langle n_{gal} \\rangle$')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a6a3a-32f5-47dc-9336-8bf5952bfe10",
   "metadata": {},
   "source": [
    "We can see in blue how the impact of this particular SP map is mitigated by applying its corresponding weight map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2533db-26c0-4931-be24-7c73f22f91a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bb404-73a8-4217-ac01-cb2112320201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895da16c-e307-4698-96c8-34510c55ce0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a8d97-7572-48e0-a083-d3c859ef46f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
